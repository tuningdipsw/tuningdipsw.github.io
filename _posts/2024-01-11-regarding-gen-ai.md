---
title: "Further reading on Generative AI"
date: 2024-01-11 14:00:00 -0000
category: meta
author: minogame
toc: true
---

["Consultation on Copyright in the Age of Generative Artificial Intelligence" (2023)](https://ised-isde.canada.ca/site/strategic-policy-sector/en/marketplace-framework-policy/consultation-paper-consultation-copyright-age-generative-artificial-intelligence)

Good day. I am writing in to express my concerns on the use of generative AI as an artist.

Although I do not have any weigh-in on how the government might regulate or enforce the use of generative AI in its legislation, I'd like to raise these concerns about generative AI for the Government of Canada to consider.

There are a number of detrimental effects to the current practice of generative AI that can be observed today. Some of these fall outside of the copyright domain that "Consultation on Copyright in the Age of Generative Artificial Intelligence" (2023) focuses on, but I will bring them up for your consideration.

My general view is that genAI in the hands of bad actors has the potential to do great harm, and that the potential benefits of genAI are narrower as its most vocal proponents would have me believe, such that I do not believe that they outweigh the negatives of removing genAI from our internet ecosystem.

However, I acknowledge the difficulties in regulating/legislating/enforcing genAI in the hands of the generic bad actor; Pandora's Box has been opened, and we cannot put genAI back in the box.

 <!--more-->

## Degradation of the trustworthiness and reliability of information that can be found on the internet

Misinformation has always been a problem with the internet, but the ease with which genAI enables the production of misinformation has only served to amplify the issue.

The modern internet has produced a financial incentive through ad revenue and search engine optimization to produce as much content as possible and push it to the forefront of our web searches, even at the detriment of accuracy. Incorrect information may even carry the risk of harm - consider information about, for example, identifying forest mushrooms that are safe to pick, foods that are safe for pets to eat, or natural remedies for health problems. Again, misinformation is not an issue that was introduced by genAI, but it has been exacerbated by it.

> "Aided by increasingly more sophisticated TDM, machine learning, and other technological advancements, AI can now create content that is difficult to distinguish from content created by human persons."

The difficulty of distinguishing AI-generated content from non- will remain a concern as long as AI-generated information remains unreliable; a first step to achieve this would be to disclose a list of sources that the content drew from. (This does not necessarily consider whether the sources were copyrighted or not, but there is some pertinence here.) "Explainability" seems to be the recognized word within the field.

(There is some further expansion on explainability and bias in this excellent article: [https://rachel.fast.ai/posts/2023-05-16-ai-centralizes-power/](https://rachel.fast.ai/posts/2023-05-16-ai-centralizes-power/))

It is an especially concerning problem with the glut of generated content that genAI may start to include unreliable genAI content in its own training datasets. To achieve reliable information, AI-generated information must eventually point to non-generated sources.

Of course, information from reputable sources can still be incorrect if poorly synthesized (or if the sources aren't used at all), but this is an improvement over information produced from the ether. (Again, this applies to human-produced information as well.)

(Data hallucination, where text generation is liable to produce "titles" with a correct syntactical format that do not actually exist, is a further extension of this issue. There was a newsworthy case in 2023 where lawyer Steven A. Schwartz's used ChatGPT in court, only to discover that it had generated citations for a number of cases that did not exist.)

It is not scalable (or even reliable) for humans to verify whether some content was or was not generated by AI, but some mechanism to identify AI-generated content is desirable. Voluntary disclosure would be helpful but is difficult to enforce. In my limited knowledge, machine tools purporting to analyze and identify AI-gen'd artwork have not been able to do so with consistency either (fittingly enough), and the reputational fallout of a false positive (witch hunting, etc) for artists has not been pretty to witness.

Generated text itself is also difficult to enforce this with due to the ease with which it may simply be copied and pasted, although I imagine that with the impact that this has had in universities with regards to academic integrity, they probably have some weigh-in on this topic. Not that I am particularly fond of the authoritative hoops that plagiarism checking software (eg. Turnitin) and their ilk have imposed on students either. Perhaps there is something to dig into regarding the harmful futility of enforcing anti-cheat policies when it comes to this genAI discussion as well, but I will leave this discussion here for now.

## Data privacy, impersonation, and defamatory concerns

Prior to genAI, it was already difficult enough to remove the traces of a photo from the internet, but the ability to diffuse someone's likeness into compromising generated images is not an improvement on the revenge porn situation.

"Deepfake porn" is discussed further here: [https://www.wired.com/story/deepfake-porn-is-out-of-control/](https://www.wired.com/story/deepfake-porn-is-out-of-control/)

I've also heard anecdotes about voice synthesis/generation being used to conduct impersonation scam phone calls (something along the lines of the Japanese "Ore Ore Sagi" scam). I believe it is even more difficult to identify generated voices than it is to identify generated images, especially if phone conditions provide poor sound quality.

To my knowledge, reasonably accurate voice synthesis can be created with surprisingly little vocal material, but you may have to investigate this on your own. This is still a concern for public-facing figures who provide a great body of material to train off of, such as actors (especially voice actors), singers, and politicians, whose voices can now easily be manipulated to produce defamatory statements.

## Concerns surrounding genAI within the Japanese creatives community

I recently read through a pair of articles by the Japanese illustrator You Shimizu, who discusses concerns with the state of AI in relation to image generation in 2023 and 2024 within the Japanese artist community. I thought their writing raised some pertinent points that may prove useful in the section afterwards. Both articles are written in Japanese, but Google Translate provides a readable translation of their core points.

"*Results of a survey of painters regarding the learning of image generation AI and future considerations*"
[https://note.com/freena_illust/n/n41043ec3a0a2](https://note.com/freena_illust/n/n41043ec3a0a2)

-> A sizable portion of polled artists did not want to have their works trained upon, even if compensation would be offered.

-> The use of publicly posted artworks as training data without permission has damaged the trust between artists and genAI users.

-> There is a need for artists' work to be protected from being trained upon through "opt-in" systems, as opposed to "opt-out" ones.

-> The threat of having their work be trained upon without their consent has led some artists to pull their work from the internet, citing concerns about how the data will be used.

-> Image generation AI provides increased "productivity", but we can't assume that this will correspond to increased profit.
"Unlike industrial products, painting is a field where it cannot be said that productivity = profit. The value of anything in excess can easily fall."

&nbsp;  

"*Summary of the image generation AI situation in 2024, future considerations, and advice for budding creators*"
[https://note.com/freena_illust/n/ne1442b0563c3](https://note.com/freena_illust/n/ne1442b0563c3)

-> This article summarizes some developments around image generation near the start of 2024.

I'd like to highlight this tweet that is cited by the article:

[https://twitter.com/ednewtonrex/status/1733187760847274197](https://twitter.com/ednewtonrex/status/1733187760847274197)

> "People often say AI should be able to train on everything, without consent, because that’s how humans learn. Why should copyright stop AI learning, but not humans?
>
> This is a false equivalence for at least two reasons.
>
> First, AI scales. A single AI, trained on all the world’s content, can produce enough output to replace the demand for much of that content. No individual human can scale in this way.
>
> Second, human learning is part of a long-established social contract. Every creator who wrote a book, or painted a picture, or composed a song, did so knowing that others would learn from it. That was priced in. This is definitively not the case with AI. Those creators did not create and publish their work in the expectation that AI systems would learn from it and then be able to produce competing content at scale. The social contract has never been in place for the act of AI training.
>
> AI training is a different proposition from human learning, based on different assumptions and with different effects. It should be treated as such."

-> The unclear copyright legal implications around AI generated work is scaring off some companies from using it.

-> The negative reception of genAI work on social media adds a threat of reputational damage to companies that use it as well.

-> These are also considerations to factor into the projected profitability of genAI.

## Broader effects of genAI on creatives' livelihoods

The sentiments within the artist community including the constant feeling of having to compete against the threat of being replaced by AI, or otherwise, to eventually resign oneself to a principle defeat of integrating it into one's tools and workflow.

As mentioned above regarding voice synthesis/generation, voice actors are especially at risk of being replaced by the use of genAI.

Translator discourse has pointed to the shift towards machine translation allowing companies to replace translators with cheaper "MTL editors".
In practice, these editors have reported that the lacking quality and nonsensical choices produced by MTL mean that it is actually easier for them to translate from the source text directly than to work with the MTL, so companies are more or less just using the promised ability of MTL to justify underpaying their translators.

Some discussion of this in these two sources by experience JP->EN translators:

[https://www.reddit.com/r/LightNovels/comments/181ct0c/official_ln_translatior_on_the_effects_of_mtl/](https://www.reddit.com/r/LightNovels/comments/181ct0c/official_ln_translatior_on_the_effects_of_mtl/)

[https://twitter.com/DistantValhalla/status/1738140236369072543](https://twitter.com/DistantValhalla/status/1738140236369072543) (thread reproduced below)

> This is either complete naivete on the part of the JP side, or corporate nonsense designed to assuage the consumer in a deceptive manner. Let me break it down below.
>
> (quoting [https://twitter.com/magus_bride/status/1738118307985912165](https://twitter.com/magus_bride/status/1738118307985912165), "The Ancient Magus Bride" 's response regarding the use of MTL in the translation of the manga. Not a thread, so please open it yourself.)
>
> "unique machine translation tech with[...]editing and proofreading by professionals."
> In practice this means Mantra Corporation will be paid big bucks to MTL manga and pay an editor a pittance to massage the garbled output into readability. Quality is in no way assured here.
>
> With post-editing machine TL, the work might as well have been originally translated in the first place. About as much or more time/labor is spent undoing mistakes made by the machine, at a fraction of the price because suits think it’s "just editing work" in comparison.

The old cry of 'automation replaces labour, you Luddites' has been used to decry the concerns of the creative community, rather unkindly. I'm not certain I can make a case that creatives are entitled to a job, but I would like to see their livelihoods protected.

I believe there does theoretically exist interesting, artistic merits to thoughtful use of generative AI, but in practice the negatives far outweigh the benefits to the creative community.

> "The Act exists to promote the creation and distribution of content, to foster investment and job creation, promote just rewards for creators, and to create a thriving marketplace that offers consumers choice and access to diverse content. [25]"

> "In addition to economic rights, the Act also grants authors certain moral rights in their works, including, where reasonable in the circumstances, a right to be attributed as the author where one's work is reproduced or used in other enumerated ways."

Although this consultation focuses on copyright concerns, I believe the unchecked use of genAI, as it is practiced today, has damaged the trust and artistic dignity of the creative sphere beyond a lack of economic recompense.

At a minimum, I believe these three restrictions would be of benefit:

- genAI must disclose that a work is AI generated
- genAI must disclose what sources were used to train/generate a work
- use of a creator's works for genAI training must be opt-in

I do not know how they can or should be enforced or legislated, but I would like you to take these factors into consideration as you consider how genAI will be used in Canada.
